{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e4c5863",
      "metadata": {
        "id": "7e4c5863"
      },
      "source": [
        "# What is Named Entity Recognition (NER)?\n",
        "Named entity recognition models are pre-trained models on a specific corpus that has already trained labels that can act as categories for labeled text.\n",
        "\n",
        "This can be as simple as identifying if a label of text represents a person. For instance, high profile figures such as __Abraham Lincoln, FDR, or George Washington__ can be labeled as a __person__.\n",
        "\n",
        "There are many extensions of what NER can do for you but overall you can think of this as a tool to help extract categories of text without manually extracting information using customized regex functions or rules-based approaches.\n",
        "\n",
        "SpaCy is an excellent NLP library for NER; documentation can be found [here](https://spacy.io/api/entityrecognizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e7478dbf",
      "metadata": {
        "id": "e7478dbf",
        "outputId": "03620bc7-22ea-4ada-f01d-472ff899dfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# if you don't have the small english spacy model downloaded on local machine, uncomment this cell and execute.\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b0b45c2",
      "metadata": {
        "id": "9b0b45c2"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy # great for visualizing entitites/tokens\n",
        "model = spacy.load(\"en_core_web_sm\") # This is a pretrained NLP pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ba578d",
      "metadata": {
        "id": "41ba578d"
      },
      "source": [
        "# Some miscellaneous information that is not too relevant to NERs..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ede555f",
      "metadata": {
        "id": "6ede555f",
        "outputId": "a8e7a8d0-0d7e-46f0-c3fa-de4cf21bdb4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.pipe_names # This is what the default model architecture looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bcfd07",
      "metadata": {
        "id": "b5bcfd07",
        "outputId": "9bfc6bd7-1f71-4809-b3ee-a7067178fbb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x20f8af002c0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x20f8af3d5e0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x20f8ad75820>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x20f8ad754c0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x20f8afb9ac0>),\n",
              " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x20f8afbf9c0>)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.pipeline # some additional information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98edfc49",
      "metadata": {
        "id": "98edfc49"
      },
      "source": [
        "# Basic demo on what an NER can do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f9e818",
      "metadata": {
        "id": "c9f9e818"
      },
      "outputs": [],
      "source": [
        "text = \"For instance, high profile figures such as Abraham Lincoln, FDR, or George Washington can be labeled as a person. I wonder what they would have thought about the USA today.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0349bf54",
      "metadata": {
        "id": "0349bf54"
      },
      "outputs": [],
      "source": [
        "# pass text into model pipeline\n",
        "processed_text = model(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4848092d",
      "metadata": {
        "id": "4848092d",
        "outputId": "fccd2acf-99bd-4d0f-e4df-bb205d0cbab0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "For instance, high profile figures such as Abraham Lincoln, FDR, or George Washington can be labeled as a person. I wonder what they would have thought about the USA today."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_text # as we can see nothing is \"out of the ordinary\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52f9d29",
      "metadata": {
        "id": "f52f9d29",
        "outputId": "50544067-50f8-470a-d3e8-b6881eb86907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on Doc object:\n",
            "\n",
            "class Doc(builtins.object)\n",
            " |  Doc(Vocab vocab, words=None, spaces=None, user_data=None, *, tags=None, pos=None, morphs=None, lemmas=None, heads=None, deps=None, sent_starts=None, ents=None)\n",
            " |  A sequence of Token objects. Access sentences and named entities, export\n",
            " |      annotations to numpy arrays, losslessly serialize to compressed binary\n",
            " |      strings. The `Doc` object holds an array of `TokenC` structs. The\n",
            " |      Python-level `Token` and `Span` objects are views of this array, i.e.\n",
            " |      they don't own the data themselves.\n",
            " |  \n",
            " |      EXAMPLE:\n",
            " |          Construction 1\n",
            " |          >>> doc = nlp(u'Some text')\n",
            " |  \n",
            " |          Construction 2\n",
            " |          >>> from spacy.tokens import Doc\n",
            " |          >>> doc = Doc(nlp.vocab, words=[\"hello\", \"world\", \"!\"], spaces=[True, False, False])\n",
            " |  \n",
            " |      DOCS: https://spacy.io/api/doc\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __bytes__(...)\n",
            " |      Doc.__bytes__(self)\n",
            " |  \n",
            " |  __getitem__(...)\n",
            " |      Get a `Token` or `Span` object.\n",
            " |      \n",
            " |      i (int or tuple) The index of the token, or the slice of the document\n",
            " |          to get.\n",
            " |      RETURNS (Token or Span): The token at `doc[i]]`, or the span at\n",
            " |          `doc[start : end]`.\n",
            " |      \n",
            " |      EXAMPLE:\n",
            " |          >>> doc[i]\n",
            " |          Get the `Token` object at position `i`, where `i` is an integer.\n",
            " |          Negative indexing is supported, and follows the usual Python\n",
            " |          semantics, i.e. `doc[-2]` is `doc[len(doc) - 2]`.\n",
            " |      \n",
            " |          >>> doc[start : end]]\n",
            " |          Get a `Span` object, starting at position `start` and ending at\n",
            " |          position `end`, where `start` and `end` are token indices. For\n",
            " |          instance, `doc[2:5]` produces a span consisting of tokens 2, 3 and\n",
            " |          4. Stepped slices (e.g. `doc[start : end : step]`) are not\n",
            " |          supported, as `Span` objects must be contiguous (cannot have gaps).\n",
            " |          You can use negative indices and open-ended ranges, which have\n",
            " |          their normal Python semantics.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#getitem\n",
            " |  \n",
            " |  __init__(...)\n",
            " |      Create a Doc object.\n",
            " |      \n",
            " |      vocab (Vocab): A vocabulary object, which must match any models you\n",
            " |          want to use (e.g. tokenizer, parser, entity recognizer).\n",
            " |      words (Optional[List[str]]): A list of unicode strings to add to the document\n",
            " |          as words. If `None`, defaults to empty list.\n",
            " |      spaces (Optional[List[bool]]): A list of boolean values, of the same length as\n",
            " |          words. True means that the word is followed by a space, False means\n",
            " |          it is not. If `None`, defaults to `[True]*len(words)`\n",
            " |      user_data (dict or None): Optional extra data to attach to the Doc.\n",
            " |      tags (Optional[List[str]]): A list of unicode strings, of the same\n",
            " |          length as words, to assign as token.tag. Defaults to None.\n",
            " |      pos (Optional[List[str]]): A list of unicode strings, of the same\n",
            " |          length as words, to assign as token.pos. Defaults to None.\n",
            " |      morphs (Optional[List[str]]): A list of unicode strings, of the same\n",
            " |          length as words, to assign as token.morph. Defaults to None.\n",
            " |      lemmas (Optional[List[str]]): A list of unicode strings, of the same\n",
            " |          length as words, to assign as token.lemma. Defaults to None.\n",
            " |      heads (Optional[List[int]]): A list of values, of the same length as\n",
            " |          words, to assign as heads. Head indices are the position of the\n",
            " |          head in the doc. Defaults to None.\n",
            " |      deps (Optional[List[str]]): A list of unicode strings, of the same\n",
            " |          length as words, to assign as token.dep. Defaults to None.\n",
            " |      sent_starts (Optional[List[Union[bool, None]]]): A list of values, of\n",
            " |          the same length as words, to assign as token.is_sent_start. Will be\n",
            " |          overridden by heads if heads is provided. Defaults to None.\n",
            " |      ents (Optional[List[str]]): A list of unicode strings, of the same\n",
            " |          length as words, as IOB tags to assign as token.ent_iob and\n",
            " |          token.ent_type. Defaults to None.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#init\n",
            " |  \n",
            " |  __iter__(...)\n",
            " |      Iterate over `Token`  objects, from which the annotations can be\n",
            " |      easily accessed. This is the main way of accessing `Token` objects,\n",
            " |      which are the main way annotations are accessed from Python. If faster-\n",
            " |      than-Python speeds are required, you can instead access the annotations\n",
            " |      as a numpy array, or access the underlying C data directly from Cython.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#iter\n",
            " |  \n",
            " |  __len__(...)\n",
            " |      The number of tokens in the document.\n",
            " |      \n",
            " |      RETURNS (int): The number of tokens in the document.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#len\n",
            " |  \n",
            " |  __reduce__ = __reduce_cython__(...)\n",
            " |      Doc.__reduce_cython__(self)\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__ = __setstate_cython__(...)\n",
            " |      Doc.__setstate_cython__(self, __pyx_state)\n",
            " |  \n",
            " |  __str__(self, /)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  __unicode__(...)\n",
            " |      Doc.__unicode__(self)\n",
            " |  \n",
            " |  char_span(...)\n",
            " |      Doc.char_span(self, int start_idx, int end_idx, label=0, kb_id=0, vector=None, alignment_mode='strict')\n",
            " |      Create a `Span` object from the slice\n",
            " |              `doc.text[start_idx : end_idx]`. Returns None if no valid `Span` can be\n",
            " |              created.\n",
            " |      \n",
            " |              doc (Doc): The parent document.\n",
            " |              start_idx (int): The index of the first character of the span.\n",
            " |              end_idx (int): The index of the first character after the span.\n",
            " |              label (uint64 or string): A label to attach to the Span, e.g. for\n",
            " |                  named entities.\n",
            " |              kb_id (uint64 or string):  An ID from a KB to capture the meaning of a\n",
            " |                  named entity.\n",
            " |              vector (ndarray[ndim=1, dtype='float32']): A meaning representation of\n",
            " |                  the span.\n",
            " |              alignment_mode (str): How character indices are aligned to token\n",
            " |                  boundaries. Options: \"strict\" (character indices must be aligned\n",
            " |                  with token boundaries), \"contract\" (span of all tokens completely\n",
            " |                  within the character span), \"expand\" (span of all tokens at least\n",
            " |                  partially covered by the character span). Defaults to \"strict\".\n",
            " |              RETURNS (Span): The newly constructed object.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#char_span\n",
            " |  \n",
            " |  copy(...)\n",
            " |      Doc.copy(self)\n",
            " |  \n",
            " |  count_by(...)\n",
            " |      Doc.count_by(self, attr_id_t attr_id, exclude=None, counts=None)\n",
            " |      Count the frequencies of a given attribute. Produces a dict of\n",
            " |              `{attribute (int): count (ints)}` frequencies, keyed by the values of\n",
            " |              the given attribute ID.\n",
            " |      \n",
            " |              attr_id (int): The attribute ID to key the counts.\n",
            " |              RETURNS (dict): A dictionary mapping attributes to integer counts.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#count_by\n",
            " |  \n",
            " |  extend_tensor(...)\n",
            " |      Doc.extend_tensor(self, tensor)\n",
            " |      Concatenate a new tensor onto the doc.tensor object.\n",
            " |      \n",
            " |              The doc.tensor attribute holds dense feature vectors\n",
            " |              computed by the models in the pipeline. Let's say a\n",
            " |              document with 30 words has a tensor with 128 dimensions\n",
            " |              per word. doc.tensor.shape will be (30, 128). After\n",
            " |              calling doc.extend_tensor with an array of shape (30, 64),\n",
            " |              doc.tensor == (30, 192).\n",
            " |  \n",
            " |  from_array(...)\n",
            " |      Doc.from_array(self, attrs, array)\n",
            " |      Load attributes from a numpy array. Write to a `Doc` object, from an\n",
            " |              `(M, N)` array of attributes.\n",
            " |      \n",
            " |              attrs (list) A list of attribute ID ints.\n",
            " |              array (numpy.ndarray[ndim=2, dtype='int32']): The attribute values.\n",
            " |              RETURNS (Doc): Itself.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#from_array\n",
            " |  \n",
            " |  from_bytes(...)\n",
            " |      Doc.from_bytes(self, bytes_data, *, exclude=tuple())\n",
            " |      Deserialize, i.e. import the document contents from a binary string.\n",
            " |      \n",
            " |              data (bytes): The string to load from.\n",
            " |              exclude (list): String names of serialization fields to exclude.\n",
            " |              RETURNS (Doc): Itself.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#from_bytes\n",
            " |  \n",
            " |  from_dict(...)\n",
            " |      Doc.from_dict(self, msg, *, exclude=tuple())\n",
            " |      Deserialize, i.e. import the document contents from a binary string.\n",
            " |      \n",
            " |              data (bytes): The string to load from.\n",
            " |              exclude (list): String names of serialization fields to exclude.\n",
            " |              RETURNS (Doc): Itself.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#from_dict\n",
            " |  \n",
            " |  from_disk(...)\n",
            " |      Doc.from_disk(self, path, *, exclude=tuple())\n",
            " |      Loads state from a directory. Modifies the object in place and\n",
            " |              returns it.\n",
            " |      \n",
            " |              path (str / Path): A path to a directory. Paths may be either\n",
            " |                  strings or `Path`-like objects.\n",
            " |              exclude (list): String names of serialization fields to exclude.\n",
            " |              RETURNS (Doc): The modified `Doc` object.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#from_disk\n",
            " |  \n",
            " |  get_lca_matrix(...)\n",
            " |      Doc.get_lca_matrix(self)\n",
            " |      Calculates a matrix of Lowest Common Ancestors (LCA) for a given\n",
            " |              `Doc`, where LCA[i, j] is the index of the lowest common ancestor among\n",
            " |              token i and j.\n",
            " |      \n",
            " |              RETURNS (np.array[ndim=2, dtype=numpy.int32]): LCA matrix with shape\n",
            " |                  (n, n), where n = len(self).\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#get_lca_matrix\n",
            " |  \n",
            " |  has_annotation(...)\n",
            " |      Doc.has_annotation(self, attr, *, require_complete=False)\n",
            " |      Check whether the doc contains annotation on a token attribute.\n",
            " |      \n",
            " |              attr (Union[int, str]): The attribute string name or int ID.\n",
            " |              require_complete (bool): Whether to check that the attribute is set on\n",
            " |                  every token in the doc.\n",
            " |              RETURNS (bool): Whether annotation is present.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#has_annotation\n",
            " |  \n",
            " |  retokenize(...)\n",
            " |      Doc.retokenize(self)\n",
            " |      Context manager to handle retokenization of the Doc.\n",
            " |              Modifications to the Doc's tokenization are stored, and then\n",
            " |              made all at once when the context manager exits. This is\n",
            " |              much more efficient, and less error-prone.\n",
            " |      \n",
            " |              All views of the Doc (Span and Token) created before the\n",
            " |              retokenization are invalidated, although they may accidentally\n",
            " |              continue to work.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#retokenize\n",
            " |              USAGE: https://spacy.io/usage/linguistic-features#retokenization\n",
            " |  \n",
            " |  set_ents(...)\n",
            " |      Doc.set_ents(self, entities, *, blocked=None, missing=None, outside=None, default=SetEntsDefault.outside)\n",
            " |      Set entity annotation.\n",
            " |      \n",
            " |              entities (List[Span]): Spans with labels to set as entities.\n",
            " |              blocked (Optional[List[Span]]): Spans to set as 'blocked' (never an\n",
            " |                  entity) for spacy's built-in NER component. Other components may\n",
            " |                  ignore this setting.\n",
            " |              missing (Optional[List[Span]]): Spans with missing/unknown entity\n",
            " |                  information.\n",
            " |              outside (Optional[List[Span]]): Spans outside of entities (O in IOB).\n",
            " |              default (str): How to set entity annotation for tokens outside of any\n",
            " |                  provided spans. Options: \"blocked\", \"missing\", \"outside\" and\n",
            " |                  \"unmodified\" (preserve current state). Defaults to \"outside\".\n",
            " |  \n",
            " |  similarity(...)\n",
            " |      Doc.similarity(self, other)\n",
            " |      Make a semantic similarity estimate. The default estimate is cosine\n",
            " |              similarity using an average of word vectors.\n",
            " |      \n",
            " |              other (object): The object to compare with. By default, accepts `Doc`,\n",
            " |                  `Span`, `Token` and `Lexeme` objects.\n",
            " |              RETURNS (float): A scalar similarity score. Higher is more similar.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#similarity\n",
            " |  \n",
            " |  to_array(...)\n",
            " |      Doc.to_array(self, py_attr_ids) -> ndarray\n",
            " |      Export given token attributes to a numpy `ndarray`.\n",
            " |              If `attr_ids` is a sequence of M attributes, the output array will be\n",
            " |              of shape `(N, M)`, where N is the length of the `Doc` (in tokens). If\n",
            " |              `attr_ids` is a single attribute, the output shape will be (N,). You\n",
            " |              can specify attributes by integer ID (e.g. spacy.attrs.LEMMA) or\n",
            " |              string name (e.g. 'LEMMA' or 'lemma').\n",
            " |      \n",
            " |              attr_ids (list[]): A list of attributes (int IDs or string names).\n",
            " |              RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row\n",
            " |                  per word, and one column per attribute indicated in the input\n",
            " |                  `attr_ids`.\n",
            " |      \n",
            " |              EXAMPLE:\n",
            " |                  >>> from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
            " |                  >>> doc = nlp(text)\n",
            " |                  >>> # All strings mapped to integers, for easy export to numpy\n",
            " |                  >>> np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
            " |  \n",
            " |  to_bytes(...)\n",
            " |      Doc.to_bytes(self, *, exclude=tuple())\n",
            " |      Serialize, i.e. export the document contents to a binary string.\n",
            " |      \n",
            " |              exclude (list): String names of serialization fields to exclude.\n",
            " |              RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
            " |                  all annotations.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#to_bytes\n",
            " |  \n",
            " |  to_dict(...)\n",
            " |      Doc.to_dict(self, *, exclude=tuple())\n",
            " |      Export the document contents to a dictionary for serialization.\n",
            " |      \n",
            " |              exclude (list): String names of serialization fields to exclude.\n",
            " |              RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
            " |                  all annotations.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#to_bytes\n",
            " |  \n",
            " |  to_disk(...)\n",
            " |      Doc.to_disk(self, path, *, exclude=tuple())\n",
            " |      Save the current state to a directory.\n",
            " |      \n",
            " |              path (str / Path): A path to a directory, which will be created if\n",
            " |                  it doesn't exist. Paths may be either strings or Path-like objects.\n",
            " |              exclude (Iterable[str]): String names of serialization fields to exclude.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#to_disk\n",
            " |  \n",
            " |  to_json(...)\n",
            " |      Doc.to_json(self, underscore=None)\n",
            " |      Convert a Doc to JSON.\n",
            " |      \n",
            " |              underscore (list): Optional list of string names of custom doc._.\n",
            " |              attributes. Attribute values need to be JSON-serializable. Values will\n",
            " |              be added to an \"_\" key in the data, e.g. \"_\": {\"foo\": \"bar\"}.\n",
            " |              RETURNS (dict): The data in spaCy's JSON format.\n",
            " |  \n",
            " |  to_utf8_array(...)\n",
            " |      Doc.to_utf8_array(self, int nr_char=-1)\n",
            " |      Encode word strings to utf8, and export to a fixed-width array\n",
            " |              of characters. Characters are placed into the array in the order:\n",
            " |                  0, -1, 1, -2, etc\n",
            " |              For example, if the array is sliced array[:, :8], the array will\n",
            " |              contain the first 4 characters and last 4 characters of each word ---\n",
            " |              with the middle characters clipped out. The value 255 is used as a pad\n",
            " |              value.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  get_extension(...) from builtins.type\n",
            " |      Doc.get_extension(type cls, name)\n",
            " |      Look up a previously registered extension by name.\n",
            " |      \n",
            " |              name (str): Name of the extension.\n",
            " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#get_extension\n",
            " |  \n",
            " |  has_extension(...) from builtins.type\n",
            " |      Doc.has_extension(type cls, name)\n",
            " |      Check whether an extension has been registered.\n",
            " |      \n",
            " |              name (str): Name of the extension.\n",
            " |              RETURNS (bool): Whether the extension has been registered.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#has_extension\n",
            " |  \n",
            " |  remove_extension(...) from builtins.type\n",
            " |      Doc.remove_extension(type cls, name)\n",
            " |      Remove a previously registered extension.\n",
            " |      \n",
            " |              name (str): Name of the extension.\n",
            " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
            " |                  removed extension.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#remove_extension\n",
            " |  \n",
            " |  set_extension(...) from builtins.type\n",
            " |      Doc.set_extension(type cls, name, **kwargs)\n",
            " |      Define a custom attribute which becomes available as `Doc._`.\n",
            " |      \n",
            " |              name (str): Name of the attribute to set.\n",
            " |              default: Optional default value of the attribute.\n",
            " |              getter (callable): Optional getter function.\n",
            " |              setter (callable): Optional setter function.\n",
            " |              method (callable): Optional method for method extension.\n",
            " |              force (bool): Force overwriting existing attribute.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#set_extension\n",
            " |              USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  from_docs(...)\n",
            " |      Doc.from_docs(docs, ensure_whitespace=True, attrs=None)\n",
            " |      Concatenate multiple Doc objects to form a new one. Raises an error\n",
            " |              if the `Doc` objects do not all share the same `Vocab`.\n",
            " |      \n",
            " |              docs (list): A list of Doc objects.\n",
            " |              ensure_whitespace (bool): Insert a space between two adjacent docs whenever the first doc does not end in whitespace.\n",
            " |              attrs (list): Optional list of attribute ID ints or attribute name strings.\n",
            " |              RETURNS (Doc): A doc that contains the concatenated docs, or None if no docs were given.\n",
            " |      \n",
            " |              DOCS: https://spacy.io/api/doc#from_docs\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  cats\n",
            " |      cats: object\n",
            " |  \n",
            " |  doc\n",
            " |  \n",
            " |  ents\n",
            " |      The named entities in the document. Returns a tuple of named entity\n",
            " |      `Span` objects, if the entity recognizer has been applied.\n",
            " |      \n",
            " |      RETURNS (tuple): Entities in the document, one `Span` per entity.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#ents\n",
            " |  \n",
            " |  has_unknown_spaces\n",
            " |      has_unknown_spaces: 'bool'\n",
            " |  \n",
            " |  has_vector\n",
            " |      A boolean value indicating whether a word vector is associated with\n",
            " |      the object.\n",
            " |      \n",
            " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#has_vector\n",
            " |  \n",
            " |  is_nered\n",
            " |  \n",
            " |  is_parsed\n",
            " |  \n",
            " |  is_sentenced\n",
            " |  \n",
            " |  is_tagged\n",
            " |  \n",
            " |  lang\n",
            " |      RETURNS (uint64): ID of the language of the doc's vocabulary.\n",
            " |  \n",
            " |  lang_\n",
            " |      RETURNS (str): Language of the doc's vocabulary, e.g. 'en'.\n",
            " |  \n",
            " |  mem\n",
            " |  \n",
            " |  noun_chunks\n",
            " |      Iterate over the base noun phrases in the document. Yields base\n",
            " |      noun-phrase #[code Span] objects, if the language has a noun chunk iterator.\n",
            " |      Raises a NotImplementedError otherwise.\n",
            " |      \n",
            " |      A base noun phrase, or \"NP chunk\", is a noun\n",
            " |      phrase that does not permit other NPs to be nested within it – so no\n",
            " |      NP-level coordination, no prepositional phrases, and no relative\n",
            " |      clauses.\n",
            " |      \n",
            " |      YIELDS (Span): Noun chunks in the document.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#noun_chunks\n",
            " |  \n",
            " |  noun_chunks_iterator\n",
            " |      noun_chunks_iterator: object\n",
            " |  \n",
            " |  sentiment\n",
            " |      sentiment: 'float'\n",
            " |  \n",
            " |  sents\n",
            " |      Iterate over the sentences in the document. Yields sentence `Span`\n",
            " |      objects. Sentence spans have no label.\n",
            " |      \n",
            " |      YIELDS (Span): Sentences in the document.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#sents\n",
            " |  \n",
            " |  spans\n",
            " |  \n",
            " |  tensor\n",
            " |      tensor: object\n",
            " |  \n",
            " |  text\n",
            " |      A unicode representation of the document text.\n",
            " |      \n",
            " |      RETURNS (str): The original verbatim text of the document.\n",
            " |  \n",
            " |  text_with_ws\n",
            " |      An alias of `Doc.text`, provided for duck-type compatibility with\n",
            " |      `Span` and `Token`.\n",
            " |      \n",
            " |      RETURNS (str): The original verbatim text of the document.\n",
            " |  \n",
            " |  user_data\n",
            " |      user_data: object\n",
            " |  \n",
            " |  user_hooks\n",
            " |      user_hooks: dict\n",
            " |  \n",
            " |  user_span_hooks\n",
            " |      user_span_hooks: dict\n",
            " |  \n",
            " |  user_token_hooks\n",
            " |      user_token_hooks: dict\n",
            " |  \n",
            " |  vector\n",
            " |      A real-valued meaning representation. Defaults to an average of the\n",
            " |      token vectors.\n",
            " |      \n",
            " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
            " |          representing the document's semantics.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#vector\n",
            " |  \n",
            " |  vector_norm\n",
            " |      The L2 norm of the document's vector representation.\n",
            " |      \n",
            " |      RETURNS (float): The L2 norm of the vector representation.\n",
            " |      \n",
            " |      DOCS: https://spacy.io/api/doc#vector_norm\n",
            " |  \n",
            " |  vocab\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __pyx_vtable__ = <capsule object NULL>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(processed_text) # Data descriptors defined here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ba6383",
      "metadata": {
        "id": "c8ba6383",
        "outputId": "8b79cc8e-defb-493d-fa45-1df67637a295"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'For instance, high profile figures such as Abraham Lincoln, FDR, or George Washington can be labeled as a person. I wonder what they would have thought about the USA today.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_text.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e28fff6",
      "metadata": {
        "id": "8e28fff6",
        "outputId": "b9c5dfd1-a03f-4429-b562-28f19cfeb129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abraham Lincoln PERSON\n",
            "FDR PERSON\n",
            "George Washington PERSON\n",
            "USA GPE\n",
            "today DATE\n"
          ]
        }
      ],
      "source": [
        "for word in processed_text.ents:\n",
        "    print(word.text, word.label_) # Geopolitical entity, i.e. countries, cities, states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a959b58",
      "metadata": {
        "id": "2a959b58",
        "outputId": "3ab03fd2-ee38-47bc-cec2-b239a22a37d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">For instance, high profile figures such as \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Abraham Lincoln\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    FDR\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", or \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    George Washington\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " can be labeled as a person. I wonder what they would have thought about the \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    USA\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    today\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(processed_text, style=\"ent\", jupyter=True) # https://spacy.io/usage/visualizers\n",
        "# style has 3 different attributes: \"dep\" for dependency parse, \"ent\" for entities, and \"span\" for specified lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ed026a",
      "metadata": {
        "id": "82ed026a",
        "outputId": "dd9b7761-7708-46d1-ce5e-3c3f1e8a3706"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'People, including fictional'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.explain(\"PERSON\") # What is this entity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8708811b",
      "metadata": {
        "id": "8708811b",
        "outputId": "1352265d-3f25-4a5b-9d8e-14dc9f9931b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.explain(\"GPE\") # What is this entity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb64a4b",
      "metadata": {
        "id": "4bb64a4b",
        "outputId": "86f9da74-64f2-4916-a7e2-922fe69dcc45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Absolute or relative dates or periods'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.explain(\"DATE\") # What is this entity?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00408c64",
      "metadata": {
        "id": "00408c64"
      },
      "source": [
        "Abraham Lincoln             |  Franklin Delano Roosevelt (FDR) | George Washington\n",
        ":-------------------------:|:-------------------------:|:-------------------------:\n",
        "<img src=\"https://github.com/SpencerPao/Natural-Language-Processing/blob/main/Named_Entity_Recognition/lincoln.jpg?raw=1\" width=\"400\"/>  |  <img src=\"https://github.com/SpencerPao/Natural-Language-Processing/blob/main/Named_Entity_Recognition/FDR.jpg?raw=1\" width=\"500\"> | <img src=\"https://github.com/SpencerPao/Natural-Language-Processing/blob/main/Named_Entity_Recognition/GW.jpg?raw=1\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c4be86",
      "metadata": {
        "id": "34c4be86"
      },
      "source": [
        "# Where in the NLP pipelines is NER typically used?\n",
        "In industry, I have typically used NER's as a categorical tool. Scale-wise, think in terms of millions of documents with tables, graphics, and most importantly thousands of words.\n",
        "\n",
        "In essence, This can be used when tagging datasets, whether it be tweets, journal articles, or webpage content, this process reduce text sparsity, adding tremendous values to your dataset(s).\n",
        "\n",
        "You can check out my [Optical Character Recognition (OCR)](https://www.youtube.com/watch?v=rCgy4d2pyyA) video I did on how to extract text from non-structured based data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce157b6",
      "metadata": {
        "id": "fce157b6"
      },
      "source": [
        "Imagine that we have thousands upon thousands of documents and we need to categorize our data so we can actually work!\n",
        "- This is where we can use an NER to help our process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b980b428",
      "metadata": {
        "id": "b980b428"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab683519",
      "metadata": {
        "id": "ab683519",
        "outputId": "a38aa929-6bf4-4929-8fbe-8c4197189f7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['data\\\\0.csv',\n",
              " 'data\\\\1.csv',\n",
              " 'data\\\\2.csv',\n",
              " 'data\\\\3.csv',\n",
              " 'data\\\\4.csv',\n",
              " 'data\\\\5.csv',\n",
              " 'data\\\\6.csv',\n",
              " 'data\\\\7.csv',\n",
              " 'data\\\\8.csv',\n",
              " 'data\\\\9.csv']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fileList = glob.glob('data/*.csv')\n",
        "fileList # in this case we only have 10 documents. But, as we can see, these datasets are unlabled."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49d0c14",
      "metadata": {
        "id": "c49d0c14"
      },
      "source": [
        "# Let's get the datasets that only refer to people.\n",
        "- This helps narrow our search tremndously without lifting much of a finger.\n",
        "    - We can rely on already built tools to help further our search for what we need and want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d197c45",
      "metadata": {
        "scrolled": true,
        "id": "4d197c45",
        "outputId": "987d32bd-3a5b-4de2-a7c4-f4abf1f597f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    0\n",
            "0   Assuming the Presidency at the depth of the Gr...\n",
            "1   Born in 1882 at Hyde Park, New York�now a nati...\n",
            "2   Following the example of his fifth cousin, Pre...\n",
            "3   In the summer of 1921, when he was 39, disaste...\n",
            "4   He was elected President in November 1932, to ...\n",
            "5   By 1935 the Nation had achieved some measure o...\n",
            "6   In 1936 he was re-elected by a top-heavy margi...\n",
            "7   Roosevelt had pledged the United States to the...\n",
            "8   When the Japanese attacked Pearl Harbor on Dec...\n",
            "9   Feeling that the future peace of the world wou...\n",
            "10  As the war drew to a close, Roosevelt�s health...\n",
            "11  The Presidential biographies on WhiteHouse.gov...\n",
            "-----\n",
            "                                                    0\n",
            "0   Abraham Lincoln became the United States� 16th...\n",
            "1   Lincoln warned the South in his Inaugural Addr...\n",
            "2   Lincoln thought secession illegal, and was wil...\n",
            "3   The son of a Kentucky frontiersman, Lincoln ha...\n",
            "4   �I was born Feb. 12, 1809, in Hardin County, K...\n",
            "5   Lincoln made extraordinary efforts to attain k...\n",
            "6   He married Mary Todd, and they had four boys, ...\n",
            "7   As President, he built the Republican Party in...\n",
            "8   Lincoln never let the world forget that the Ci...\n",
            "9   Lincoln won re-election in 1864, as Union mili...\n",
            "10  The spirit that guided him was clearly that of...\n",
            "11  On Good Friday, April 14, 1865, Lincoln was as...\n",
            "12  The Presidential biographies on WhiteHouse.gov...\n",
            "-----\n",
            "                                                    0\n",
            "0   On April 30, 1789, George Washington, standing...\n",
            "1   Born in 1732 into a Virginia planter family, h...\n",
            "2   He pursued two intertwined interests: military...\n",
            "3   From 1759 to the outbreak of the American Revo...\n",
            "4   When the Second Continental Congress assembled...\n",
            "5   He realized early that the best strategy was t...\n",
            "6   Washington longed to retire to his fields at M...\n",
            "7   He did not infringe upon the policy making pow...\n",
            "8   To his disappointment, two parties were develo...\n",
            "9   Washington enjoyed less than three years of re...\n",
            "10  The Presidential biographies on WhiteHouse.gov...\n",
            "-----\n",
            "                                                   0\n",
            "0  Domino's Pizza Inc.[6] doing business as Domin...\n",
            "-----\n",
            "                                                   0\n",
            "0               What is natural language processing?\n",
            "1  Natural language processing (NLP) is the abili...\n",
            "2  NLP has existed for more than 50 years and has...\n",
            "3         How does natural language processing work?\n",
            "4  NLP enables computers to understand natural la...\n",
            "5  There are two main phases to natural language ...\n",
            "-----\n",
            "                             0\n",
            "0  Like Comment and Subscribe!\n",
            "-----\n",
            "                                                   0\n",
            "0                           What is computer vision?\n",
            "1  Computer vision is a field of artificial intel...\n",
            "2  Computer vision works much the same as human v...\n",
            "3  Computer vision trains machines to perform the...\n",
            "-----\n",
            "                                                   0\n",
            "0                        What is product management?\n",
            "1  Product management is an organizational functi...\n",
            "2  Thanks to this focus on the customer, product ...\n",
            "-----\n",
            "                                                   0\n",
            "0  Project management is the use of specific know...\n",
            "-----\n",
            "                                          0\n",
            "0  I hope you are having a wonderful day :)\n",
            "1                See you in the next video!\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# Just for our sake, we will only be looking at the first row.\n",
        "# If true, then label dataset as store file names to be used later on and further preprocess.\n",
        "for f in fileList:\n",
        "    file_read = pd.read_csv(f, header = None)\n",
        "    print(file_read)\n",
        "    print(\"-----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb78733b",
      "metadata": {
        "scrolled": false,
        "id": "fb78733b",
        "outputId": "0e290ba0-c55c-48e5-dab1-c0fde6ecda60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text being read in..... for file data\\0.csv\n",
            "Assuming the Presidency at the depth of the Great Depression, Franklin D. Roosevelt helped the American people regain faith in themselves. He brought hope as he promised prompt, vigorous action, and asserted in his Inaugural Address, �the only thing we have to fear is fear itself.�\n",
            "----- Checking if there are any PERSON entitities\n",
            "the Great Depression EVENT\n",
            "Franklin D. Roosevelt PERSON\n",
            "American NORP\n",
            "Inaugural Address ORG\n",
            "-----\n",
            "Text being read in..... for file data\\1.csv\n",
            "Abraham Lincoln became the United States� 16th President in 1861, issuing the Emancipation Proclamation that declared forever free those slaves within the Confederacy in 1863.\n",
            "----- Checking if there are any PERSON entitities\n",
            "Abraham Lincoln PERSON\n",
            "the United States GPE\n",
            "16th ORDINAL\n",
            "1861 DATE\n",
            "the Emancipation Proclamation FAC\n",
            "1863 DATE\n",
            "-----\n",
            "Text being read in..... for file data\\2.csv\n",
            "On April 30, 1789, George Washington, standing on the balcony of Federal Hall on Wall Street in New York, took his oath of office as the first President of the United States. �As the first of every thing, in our situation will serve to establish a Precedent,� he wrote James Madison, �it is devoutly wished on my part, that these precedents may be fixed on true principles.�\n",
            "----- Checking if there are any PERSON entitities\n",
            "April 30, 1789 DATE\n",
            "George Washington PERSON\n",
            "Federal Hall on Wall Street ORG\n",
            "New York GPE\n",
            "first ORDINAL\n",
            "the United States GPE\n",
            "first ORDINAL\n",
            "James Madison PERSON\n",
            "-----\n",
            "Text being read in..... for file data\\3.csv\n",
            "Domino's Pizza Inc.[6] doing business as Domino's is an American multinational pizza restaurant chain founded in 1960 and led by CEO Richard Allison. The corporation is Delaware domiciled[7] and headquartered at the Domino's Farms Office Park in Ann Arbor, Michigan.[3][5] As of 2018, Domino's had approximately 15,000 stores, with 5,649 in the United States, 1,500 in India, and 1,249 in the United Kingdom.[8][9] Domino's has stores in over 83 countries[10] and 5,701 cities worldwide.[11] In 2018 Domino's Pizza was inducted into the Queensland Business Leaders Hall of Fame.[12]\n",
            "----- Checking if there are any PERSON entitities\n",
            "Domino PERSON\n",
            "Domino PERSON\n",
            "American NORP\n",
            "1960 DATE\n",
            "Richard Allison PERSON\n",
            "Delaware GPE\n",
            "Domino PERSON\n",
            "Ann Arbor GPE\n",
            "Michigan.[3][5 PRODUCT\n",
            "2018 DATE\n",
            "Domino PERSON\n",
            "approximately 15,000 CARDINAL\n",
            "5,649 CARDINAL\n",
            "the United States GPE\n",
            "1,500 CARDINAL\n",
            "India GPE\n",
            "1,249 CARDINAL\n",
            "Domino PERSON\n",
            "5,701 CARDINAL\n",
            "2018 DATE\n",
            "Domino PERSON\n",
            "the Queensland Business Leaders Hall ORG\n",
            "-----\n",
            "Text being read in..... for file data\\4.csv\n",
            "What is natural language processing?\n",
            "----- Checking if there are any PERSON entitities\n",
            "-----\n",
            "Text being read in..... for file data\\5.csv\n",
            "Like Comment and Subscribe!\n",
            "----- Checking if there are any PERSON entitities\n",
            "-----\n",
            "Text being read in..... for file data\\6.csv\n",
            "What is computer vision?\n",
            "----- Checking if there are any PERSON entitities\n",
            "-----\n",
            "Text being read in..... for file data\\7.csv\n",
            "What is product management?\n",
            "----- Checking if there are any PERSON entitities\n",
            "-----\n",
            "Text being read in..... for file data\\8.csv\n",
            "Project management is the use of specific knowledge, skills, tools and techniques to deliver something of value to people. The development of software for an improved business process, the construction of a building, the relief effort after a natural disaster, the expansion of sales into a new geographic market�these are all examples of projects.\n",
            "----- Checking if there are any PERSON entitities\n",
            "Project ORG\n",
            "-----\n",
            "Text being read in..... for file data\\9.csv\n",
            "I hope you are having a wonderful day :)\n",
            "----- Checking if there are any PERSON entitities\n",
            "-----\n",
            "Our current files that have the PERSON entity labeled in the first paragraph.: \n",
            "['data\\\\0.csv', 'data\\\\1.csv', 'data\\\\2.csv', 'data\\\\3.csv']\n"
          ]
        }
      ],
      "source": [
        "# Let's see how we can use NER in action.\n",
        "white_list_files = []\n",
        "for f in fileList:\n",
        "    file_read = pd.read_csv(f, header = None)\n",
        "    print(f\"Text being read in..... for file {f}\")\n",
        "    print(file_read.values[0][0])\n",
        "    processed_text = model(file_read.values[0][0])\n",
        "    print(\"----- Checking if there are any PERSON entitities\")\n",
        "    is_people = False\n",
        "    for word in processed_text.ents:\n",
        "        print(word.text, word.label_)\n",
        "        if word.label_ == \"PERSON\":\n",
        "            is_people = True\n",
        "    if is_people:\n",
        "        white_list_files.append(f)\n",
        "    print(\"-----\")\n",
        "\n",
        "print(\"Our current files that have the PERSON entity labeled in the first paragraph.: \")\n",
        "print(white_list_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97bd99a",
      "metadata": {
        "id": "d97bd99a"
      },
      "source": [
        "# Customize YOUR NER!\n",
        "- Of course, there is always manual work once you get into the details\n",
        "- This is a quick demonstration on how to tune your very own NER to fit your use cases!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c237c042",
      "metadata": {
        "id": "c237c042"
      },
      "outputs": [],
      "source": [
        "raw_text = \"CP30 and R2D2 are the droids we are looking for! Let's assume they are people for the sake of argument.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad537134",
      "metadata": {
        "id": "ad537134"
      },
      "outputs": [],
      "source": [
        "processed_text = model(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e051f2c",
      "metadata": {
        "id": "0e051f2c",
        "outputId": "2a939c13-afc9-4809-ab8a-adce8db50bda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\users\\spenc\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\displacy\\__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
            "  warnings.warn(Warnings.W006)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">CP30 and R2D2 are the droids we are looking for! Let's assume they are people for the sake of argument.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(processed_text, style=\"ent\", jupyter=True) # As we can see, Cp30 and R2D2 are not labled as \"DROID\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c207843",
      "metadata": {
        "id": "4c207843"
      },
      "source": [
        "### So, how do we label new entities to an already existing model?\n",
        "- A basic example on how to this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d90dc4",
      "metadata": {
        "id": "51d90dc4"
      },
      "outputs": [],
      "source": [
        "from spacy.tokens import Span"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e15673",
      "metadata": {
        "id": "d3e15673",
        "outputId": "d62b0947-38a6-4182-98d9-2c38845b0157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "380\n"
          ]
        }
      ],
      "source": [
        "PERSON = processed_text.vocab.strings[u'PERSON'] # hashvalue of the DROID entity (This is a new entity)\n",
        "print(PERSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d591832",
      "metadata": {
        "id": "3d591832"
      },
      "outputs": [],
      "source": [
        "entity_CP30 = Span(processed_text, 0, 3, label = PERSON) # processed text, start position, end position, label\n",
        "entity_R2D2 = Span(processed_text, 9, 12, label = PERSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb5033f",
      "metadata": {
        "id": "5fb5033f"
      },
      "outputs": [],
      "source": [
        "processed_text.ents = list(processed_text.ents) + [entity_CP30] + [entity_R2D2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d7824f",
      "metadata": {
        "id": "41d7824f",
        "outputId": "d8c20efd-6a81-4bd9-c838-ed7f34348083"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    CP30 and R2D2\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " are the droids we are looking \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    for! Let\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "'s assume they are people for the sake of argument.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(processed_text, style=\"ent\", jupyter=True) # As we can see, Cp30 and R2D2 are not labled as \"DROID\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796cc9e2",
      "metadata": {
        "id": "796cc9e2"
      },
      "source": [
        "# Well, how but adding entirely new entities?\n",
        "That's for a different video/notebook. So, let me know if you are interested in that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d848b1a7",
      "metadata": {
        "id": "d848b1a7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0e6162",
      "metadata": {
        "id": "fc0e6162"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2378eeaf",
      "metadata": {
        "id": "2378eeaf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "21d57bf2",
      "metadata": {
        "id": "21d57bf2"
      },
      "source": [
        "# That's cool! What's next?\n",
        "- You can check out all sorts of models that have an NER built into its model architecture. In fact, a great repository is [Huggingface](https://huggingface.co/) where all sorts of NLP models are stored publicy. They have models from all different types of industries! Might be worth to check it out."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}